<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training · Data Parallel Training</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Data Parallel Training</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Training</a><ul class="internal"><li><a class="tocitem" href="#Syncing-Gradients"><span>Syncing Gradients</span></a></li><li><a class="tocitem" href="#High-Level-training-pipeline"><span>High Level training pipeline</span></a></li></ul></li><li><a class="tocitem" href="../datasets/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Training</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/DhairyaLGandhi/ResNetImageNet.jl/blob/master/docs/src/training.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-pipeline"><a class="docs-heading-anchor" href="#Training-pipeline">Training pipeline</a><a id="Training-pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#Training-pipeline" title="Permalink"></a></h1><p>Data parallel training happens over several GPUs which themselves may be spread across sevral nodes. While there are several architectures proposed for high throughput and fast training of neural networks with a large amount of data, it is out of scope for this document. Here we will focus on the tooling necessary to achieve training over several GPUs and the API.</p><h2 id="Syncing-Gradients"><a class="docs-heading-anchor" href="#Syncing-Gradients">Syncing Gradients</a><a id="Syncing-Gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Syncing-Gradients" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ResNetImageNet.syncgrads" href="#ResNetImageNet.syncgrads"><code>ResNetImageNet.syncgrads</code></a> — <span class="docstring-category">Function</span></header><section><div><p>syncgrads(input<em>channels, output</em>channels; verbose = false)</p><p>Starts a task to monitor all the <code>input_channels</code> to receive a signal and performs synchronisation of all the terms in the input channels.</p><p>The gradients from these channels are expected to be of the form of a <code>NamedTuple</code> as produced by Zygote. A typical example would be</p><pre><code class="language-julia hljs">julia&gt; resnet = ResNet(); # from Metalhead.jl or could be any model we wish to train

julia&gt; using Zygote

julia&gt; gs, _ = gradient(resnet, rand(Float32, 224, 224, 3, 1)) do m, x
         sum(m(x))
       end;</code></pre><p><code>gs</code> is what would be sent in the channels from every worker.</p><p>All the input channels are expected to be started at the remote processes with a size 1 so only one gradient may be published at one time. The output channels are similar with the channel being started on the process where the sync in expected to happen.</p><p>Typical configuration would look like:</p><pre><code class="language-julia hljs">input_channels = (RemoteChannel(() -&gt; Channel(1), p) for p in workers())
output_channels = (RemoteChannel(() -&gt; Channel(1), 1) for p in workers())</code></pre><p>Set <code>verbose = true</code> to get more detailed information as the synchronisation happens.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DhairyaLGandhi/ResNetImageNet.jl/blob/4c41fad611de3b641a965ce25ce2a59bbb56ac64/src/sync.jl#L6-L35">source</a></section></article><p>Note that <code>syncgrads</code> currently requires serialization of gradients from every device with every iteration of the data loader. This is inefficient and has been surpassed with techniques involving &quot;data layers&quot; such as Nvidia NCCL or UCX which work to perform reduction over several GPUs in a better optimised manner. This is under developement in the Julia ecosystem as well.</p><h2 id="High-Level-training-pipeline"><a class="docs-heading-anchor" href="#High-Level-training-pipeline">High Level training pipeline</a><a id="High-Level-training-pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-training-pipeline" title="Permalink"></a></h2><p>A basic pipeline would look very similar to how <code>Flux.train!</code> functions. We need a cost function, some data and parameters and we are off to the races. There are some subtlties however. Rather than using the gradients that every copy of the model produces with the data it used, data parallel training does not directly use these gradients. Instead, the gradients from every model have to be reduced together to average it out to get the effect of training over all the data used to perform a single training step. This amounts to:</p><pre><code class="language-julia hljs"># state =&gt; state of optimiser
# model =&gt; model to be trained
# opt =&gt; optimiser
# data =&gt; collection of data
for d in data
  x, y = d  # x =&gt; input data, y =&gt; labels
  gs = gradient(model) do m
    loss(m(x), y)
  end

  # note the pseudo code call in the next line
  @synchronize_gradients_over_all_GPUs
  model, state = opt(model, updated_grads, state)
end</code></pre><p>This looks very similar to the typical supervised learning training loop from Flux.</p><p>In fact, it is! With the addition of the synchrnisation part, we can also extend it to several forms of semi-supervised and unsupervised learning scenarios. This is part of the future work of this pacakge, and something actively being researched in the Julia community and elsewhere.</p><article class="docstring"><header><a class="docstring-binding" id="ResNetImageNet.start" href="#ResNetImageNet.start"><code>ResNetImageNet.start</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">start(loss, data_tree, key, model,
      input_channels, output_channels;
      class_idx,
      verbose = false,
      opt_args = (),
      opt = Optimisers.ADAM(),
      kwargs...)</code></pre><p>The high level function that performs training of the model over the specified data and configuration.</p><ul><li><p><code>loss</code>: Typical loss function used to optimise the model with the data. It is fed all the data that every iteration of a <code>DataLaader</code> produces, such that the first element of the produced data is first send to the model. The calling signature of the loss looks like:</p><p>x, y, z... = iterate(dataloader)   loss(model(x), y, z...)</p></li><li><p><code>data_tree</code>: the data tree that one would associate with a dataset described by <code>DataSets.dataset</code>.</p></li><li><p><code>key</code>: the key  to the training data. Typically the LOC_train__solutions.csv` for the case of ILSVRC.</p></li><li><p><code>model</code>: model to be trained</p></li><li><p>input<em>channels, output</em>channels: See <a href="@ref"><code>syncgrad</code></a> for details</p></li></ul><p>Keyword Arguments:</p><p>This is a non-exhaustive list of keyword arguments currently supported.</p><ul><li>class_idx: a list of the labels to be trained on, a Vector, or Base.OneTo</li><li>o: The type of optimiser to use. Optimisers.jl provides a number of supported optimisers.</li><li>opt_args: A tuple containing arguments to the optimiser as described by <code>o</code></li><li>opt: The output of <code>o(opt_args...)</code>. Useful to provide initial optimisers. Can also be associated with schedulers and decays as required.</li><li>cycles: the number of times the dataset is sampled in a single call to <code>start</code>.</li><li>batchsize: the number of observations per batch per GPU.</li><li>nsamples: The number of datapoints to be sampled at once. This subset of data is loaded by every process independently and creates a <code>DataLoader</code> from it.</li><li>sts: A vector of length <code>nworkers()</code> which each contain the current state of the optimiser. This is initialised as <code>[Optimisers.state(opt, model) for _ in workers()]</code></li><li>saveweights: Defaults to <code>false</code>, set to <code>true</code> to save training checkpoints</li><li>vals: A vector of validation sets to be used as validation sets while training. These also add logging statements while training. Disable with (and defaults to) <code>[nothing for _ in workers()]</code>. </li><li>workers: A list of processes used to train the model</li><li>devices: A <code>DeviceSet()</code> or iterable of <code>Device()</code>; used to target the GPU used to train.</li><li>verbose: Defaults to <code>false</code>. Set to <code>true</code> to enable logging of helpful information while debugging and training.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DhairyaLGandhi/ResNetImageNet.jl/blob/4c41fad611de3b641a965ce25ce2a59bbb56ac64/src/sync.jl#L174-L215">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../datasets/">Datasets »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.6 on <span class="colophon-date" title="Wednesday 8 September 2021 14:03">Wednesday 8 September 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
