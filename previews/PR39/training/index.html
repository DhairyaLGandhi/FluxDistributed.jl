<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training · Data Parallel Training</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Data Parallel Training</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Training</a><ul class="internal"><li><a class="tocitem" href="#Batching-the-Data"><span>Batching the Data</span></a></li><li><a class="tocitem" href="#Syncing-Gradients"><span>Syncing Gradients</span></a></li><li><a class="tocitem" href="#High-Level-training-pipeline"><span>High Level training pipeline</span></a></li></ul></li><li><a class="tocitem" href="../datasets/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Training</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/DhairyaLGandhi/ResNetImageNet.jl/blob/master/docs/src/training.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-pipeline"><a class="docs-heading-anchor" href="#Training-pipeline">Training pipeline</a><a id="Training-pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#Training-pipeline" title="Permalink"></a></h1><p>Data parallel training happens over several GPUs which themselves may be spread across sevral nodes. While there are several architectures proposed for high throughput and fast training of neural networks with a large amount of data, detailing them is out of scope for this document. Here we will focus on the tooling necessary to achieve training over several GPUs and the API.</p><h2 id="Batching-the-Data"><a class="docs-heading-anchor" href="#Batching-the-Data">Batching the Data</a><a id="Batching-the-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Batching-the-Data" title="Permalink"></a></h2><p>There are several strategies that can be employed for parallel loading of data. Typically, the process involves sharding of data by the number of accelerator units, and creating data loaders which can load data from the disk asynchronously with the training loop. In order to serve all the accelerators, one can call a <code>DataParallelDataLoader</code> or create <code>N</code> instances of iterable producing mini-batches (where <code>N</code> refers to the number of accelerator units).</p><p>In this package, the <code>prepare_training</code> function uses a modified version of a Flux DataLoader which can simultaneously feed <code>N</code> accelerators, load data and move it to the accelerator in parallel with the training. This way, one can write custom loading and preprocessing scripts to be run in parallel with the training, and evern overlapping network costs to move data to the accelerator without training loop needing to wait for the data to be available to it. Data is managed through DataSets.jl (it is also useful for setting up any custom dataset, and more information can be found in the <a href="../datasets.md">datasets document</a>.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ResNetImageNet.prepare_training</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Syncing-Gradients"><a class="docs-heading-anchor" href="#Syncing-Gradients">Syncing Gradients</a><a id="Syncing-Gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Syncing-Gradients" title="Permalink"></a></h2><p>Distributed training requires the gradients to be synchronized during the process of training. Using this, a model can get the effect of training over the cumulative data used by every accelerator at every step. This is done for the following reasons:</p><ul><li><code>N</code> instances of the training pipeline are instantiated on <code>N</code> accelerators.</li><li>Every accelerator then injests data independently and produces gradients corresponding to its specific sub-batch of data.</li><li>By reducing the gradients over all the instances, we can simulate training over all the sub-batches at once.<ul><li>By default, the gradients are averaged over all avaliable instances of the model, but this behaviour can be customised in <a href="."><code>ResNetImageNet.sync_buffer</code></a></li></ul></li></ul><h3 id="Single-Node-Parallelism"><a class="docs-heading-anchor" href="#Single-Node-Parallelism">Single Node Parallelism</a><a id="Single-Node-Parallelism-1"></a><a class="docs-heading-anchor-permalink" href="#Single-Node-Parallelism" title="Permalink"></a></h3><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ResNetImageNet.sync_buffer</code>. Check Documenter&#39;s build log for details.</p></div></div><p><code>sync_buffer</code> currently requires maintaining preallocated memory on one of the accelerator units in order to not pay the price for allocations with every synchnorization step. It makes a call to <code>copyto!</code> and in case the accelerators are not connected via a P2P mechanism, can cause implicit serialization to the CPU, hurting performance significantly.</p><h3 id="Multi-Node-Parallelism"><a class="docs-heading-anchor" href="#Multi-Node-Parallelism">Multi Node Parallelism</a><a id="Multi-Node-Parallelism-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Node-Parallelism" title="Permalink"></a></h3><p>!!! Note     The multi-node parallelism pipeline is currently disabled, but available in the repository for expermential purposes via <code>ResNetImageNet.syncgrads</code>.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ResNetImageNet.syncgrads</code>. Check Documenter&#39;s build log for details.</p></div></div><p>Note that <code>syncgrads</code> currently requires serialization of gradients from every device with every iteration of the data loader. This is inefficient and has been surpassed with techniques involving &quot;data layers&quot; such as Nvidia NCCL or UCX which work to perform reduction over several GPUs in a better optimised manner. This is under developement in the Julia ecosystem as well.</p><h2 id="High-Level-training-pipeline"><a class="docs-heading-anchor" href="#High-Level-training-pipeline">High Level training pipeline</a><a id="High-Level-training-pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-training-pipeline" title="Permalink"></a></h2><p>A basic pipeline would look very similar to how <code>Flux.train!</code> functions. We need a cost function, some data and parameters and we are off to the races. There are some subtlties however. Rather than using the gradients that every copy of the model produces with the data it used, data parallel training does not directly use these gradients. Instead, the gradients from every model have to be reduced together to average it out to get the effect of training over all the data used to perform a single training step. This amounts to:</p><pre><code class="language-julia hljs"># state =&gt; state of optimiser
# model =&gt; model to be trained
# opt =&gt; optimiser
# data =&gt; collection of data
for d in data
  x, y = d  # x =&gt; input data, y =&gt; labels
  gs = gradient(model) do m
    loss(m(x), y)
  end

  # note the pseudo code call in the next line
  @synchronize_gradients_over_all_GPUs
  model, state = opt(model, updated_grads, state)
end</code></pre><p>This looks very similar to the typical supervised learning training loop from Flux.</p><p>In fact, it is! With the addition of the synchronization part, we can also extend it to several forms of semi-supervised and unsupervised learning scenarios. This is part of the future work of this pacakge, and something actively being researched in the Julia community and elsewhere.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ResNetImageNet.train_step</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ResNetImageNet.update</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ResNetImageNet.train</code>. Check Documenter&#39;s build log for details.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../datasets/">Datasets »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.14 on <span class="colophon-date" title="Wednesday 16 March 2022 21:21">Wednesday 16 March 2022</span>. Using Julia version 1.6.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
